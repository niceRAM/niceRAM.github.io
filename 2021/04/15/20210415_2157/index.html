<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="chrome=1"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><meta name=author content="niceRAM"><meta name=description content="niceRAM's Blog"><link rel=prev href=https://niceram.xyz/2021/03/25/20210325_1319/><link rel=next href=https://niceram.xyz/2021/07/07/20210707_1441/><link rel=canonical href=https://niceram.xyz/2021/04/15/20210415_2157/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#da532c"><meta name=theme-color content="#ffffff"><title>Docker + Miniconda + VSCode Remote SSH 在 Ubuntu 搭建多版本共存的 TensorFlow-GPU 远程开发环境 | NiceBlog</title><meta name=title content="Docker + Miniconda + VSCode Remote SSH 在 Ubuntu 搭建多版本共存的 TensorFlow-GPU 远程开发环境 | NiceBlog"><link rel=stylesheet href=/css/main.min.css><link rel=stylesheet href=/font/iconfont.css><link rel=stylesheet href=/font-ali/iconfont.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/niceram.xyz\/"},"articleSection":"posts","name":"Docker \u002b Miniconda \u002b VSCode Remote SSH 在 Ubuntu 搭建多版本共存的 TensorFlow-GPU 远程开发环境","headline":"Docker \u002b Miniconda \u002b VSCode Remote SSH 在 Ubuntu 搭建多版本共存的 TensorFlow-GPU 远程开发环境","description":"工作原因，需要用到 TensorFlow-GPU， 之前一直是机器装好了 Nvidia 驱动、CUDA、cudnn 后紧着一个 TensorFlow 版本一直用， 有创建虚拟环境的需求","inLanguage":"zh-CN","author":"niceRAM","creator":"niceRAM","publisher":"niceRAM","accountablePerson":"niceRAM","copyrightHolder":"niceRAM","copyrightYear":"2021","datePublished":"2021-04-15 21:58:06 \u002b0800 \u002b0800","dateModified":"2021-04-15 21:58:06 \u002b0800 \u002b0800","url":"https:\/\/niceram.xyz\/2021\/04\/15\/20210415_2157\/","wordCount":"4391","keywords":["docker","miniconda","anaconda","tensorflow","tensorflow-gpu","ubuntu","Nvidia","CUDA","cudnn","CUDA 多版本共存","NiceBlog"]}</script></head><body><div class=wrapper><nav class=navbar><progress class=content_progress max=0 value=0></progress><div class=container><div class="navbar-header header-back2home-logo"><span class=logo_mark>>$</span>
<a href=https://niceram.xyz/><span class=logo_text>cd /home/</span>
<span class=logo_cursor></span></a></div><div class=navbar-right><span class=menu><a class=menu-item href=/posts/ title>📚Blog</a>
<a class=menu-item href=/categories/ title>📂Categories</a>
<a class=menu-item href=/tags/ title>📌Tags</a>
<a class=menu-item href=/about/ title>💬About</a>
<span class=divide></span><a href=javascript:void(0); class=theme-switch><i class="iconfont icon-dark-mode"></i></a></span></div></div></nav><nav class=navbar-mobile id=nav-mobile style=display:none><progress class=content_progress max=0 value=0></progress><div class=container><div class=navbar><div class="navbar-header header-logo"><a href=https://niceram.xyz/>NiceBlog</a></div><div class=navbar-right><div><a href=javascript:void(0); class=theme-switch><i class="iconfont icon-dark-mode"></i></a></div><div class=menu-toggle><span></span><span></span><span></span></div></div></div><div class=menu id=mobile-menu><nav class=mb-md><a class=menu-item href=/posts/ title><h3>📚Blog</h3><div class=menu-active></div></a><a class=menu-item href=/categories/ title><h3>📂Categories</h3><div class=menu-active></div></a><a class=menu-item href=/tags/ title><h3>📌Tags</h3><div class=menu-active></div></a><a class=menu-item href=/about/ title><h3>💬About</h3><div class=menu-active></div></a></nav></div></div></nav><main class=main><div class=container><article class=post-warp itemscope itemtype=http://schema.org/Article><header class=post-header><h1 class=post-title itemprop="name headline">Docker + Miniconda + VSCode Remote SSH 在 Ubuntu 搭建多版本共存的 TensorFlow-GPU 远程开发环境</h1><div class=post-meta>Written by <a itemprop=name href=https://niceram.xyz/ rel=author>niceRAM</a> with ♥
<span class=post-time>on <time datetime=2021-04-15 itemprop=datePublished>April 15, 2021</time></span>
in
<i class="iconfont icon-folder"></i><span class=post-category><a href=https://niceram.xyz/categories/%E6%87%92%E6%8E%A8%E5%8A%A8%E6%8A%80%E6%9C%AF%E8%BF%9B%E6%AD%A5/>懒推动技术进步,</a></span>
<span class=post-word-count>4391 words</span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title></h2><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#背景>背景</a><ul><li><a href=#为什么需要多版本共存>为什么需要多版本共存</a></li><li><a href=#时下的解决方案>时下的解决方案</a></li><li><a href=#现有方案的缺陷>现有方案的缺陷</a></li><li><a href=#亿点废话>亿点废话</a></li></ul></li><li><a href=#事前准备>事前准备</a><ul><li><a href=#安装-docker-engine>安装 Docker Engine</a></li><li><a href=#安装-docker-compose>安装 Docker-Compose</a></li><li><a href=#安装-nvidia-container-toolkit>安装 nvidia-container-toolkit</a></li><li><a href=#下载-tensorflow-的-docker-镜像>下载 TensorFlow 的 Docker 镜像</a></li></ul></li><li><a href=#骚操作的正式开始>骚操作的正式开始</a><ul><li><a href=#编写-dockerfile>编写 Dockerfile</a></li><li><a href=#dockerfile-需要用到的脚本>Dockerfile 需要用到的脚本</a></li><li><a href=#打包镜像>打包镜像</a></li><li><a href=#新建容器并启动>新建容器并启动</a></li><li><a href=#安装-minicondaanaconda>安装 Miniconda/Anaconda</a></li><li><a href=#使用-docker-compose-管理容器>使用 Docker-Compose 管理容器</a></li></ul></li><li><a href=#骚操作的后续补充>骚操作的后续补充</a><ul><li><a href=#为容器初始化-conda>为容器初始化 conda</a></li><li><a href=#简单的使用说明>简单的使用说明</a></li><li><a href=#使用-vscode-远程开发>使用 VSCode 远程开发</a></li></ul></li><li><a href=#一些已知存在但仍不知如何解决的小问题>一些已知存在但仍不知如何解决的小问题</a></li></ul></nav></div></div><div class=post-content><img src=https://cdn.jsdelivr.net/gh/niceRAM/blog-images/img/20210415_2157/2021/04/16/14-10-57-899-f0d5b7.png class=featured_image><p>工作原因，需要用到 TensorFlow-GPU，
之前一直是机器装好了 Nvidia 驱动、CUDA、cudnn 后紧着一个 TensorFlow 版本一直用，
有创建虚拟环境的需求就用 virtualenv 新建一个环境，再装上 TensorFlow-GPU 和一堆依赖。</p><p>这个过程虽然操作不难，但重复工作着实无聊，而且 virtualenv 管理虚拟环境并不方便，
而更重要的是<strong>虚拟环境并不能解决同一 OS 上 TensorFlow 和 CUDA 的多版本共存问题</strong>！</p><p>最近组内的服务器重装系统，刚好最近也没那么忙了，环境重建也是由我处理，
就顺手多学点容器技术，自己琢磨了一套感觉用着比较舒服的解决方案。</p><hr><h2 id=背景>背景</h2><h3 id=为什么需要多版本共存>为什么需要多版本共存</h3><p>安装的 TensorFlow 版本不同，所需的 CUDA 版本也有可能不一样，
所以不同项目依赖了不同版本的 TensorFlow，就可能需要安装对应版本的 CUDA。</p><p>但是因为 CUDA 的安装会修改软链接 <code>/usr/local/cuda/</code> 指向的路径，也就会改变了当前系统的默认 CUDA。
所以在没有人为干涉的情况下，系统同时只能识别到一个版本的 CUDA，也就是最后安装的 CUDA。</p><p>因此依赖其他版本 CUDA 的 TensorFlow 就无法正常使用 GPU 了。</p><h3 id=时下的解决方案>时下的解决方案</h3><p>此前遇到 TensorFlow-GPU 共存的问题通常有这几个解决方案：</p><ol><li>按照正常流程安装需要的 TensorFLow1/TensorFlow2 以及所有对应版本的 CUDA，
通过脚本在切换虚拟环境时指定要使用的 CUDA。</li><li>不论是 TensorFLow1 还是 TensorFlow2，大版本几乎没有不向下兼容的情况，
所以尽量安装高版本的 TensorFlow1，然后根据已安装的 TensorFlow1 依赖的 CUDA 版本，
在其他虚拟环境安装同样依赖该 CUDA 版本的 TensorFlow2。</li><li>使用 TensorFlow 官方提供的 Docker 镜像。</li></ol><h3 id=现有方案的缺陷>现有方案的缺陷</h3><ol><li>需要安装多个CUDA、写脚本……具体我没有成功实施过，单纯是安装 CUDA 就因为网络等原因失败多次，
具体有什么缺陷，不太好说。</li><li>一旦遇到需要更高版本的 TensorFlow2 或者 更低版本的 TensorFlow1 的项目，这个方案就无法应对。</li><li>这里我认为最简单、最合理的方式是第 3 个。但是仍然存在局限性：<ul><li>每个环境独立的项目都要新建一个容器，也可能需要下载其他版本 TensorFlow 的镜像，
而每一个镜像都有 3~4GB 那么大。
(btw，重装系统前，我们的服务器是双系统的，而 Ubuntu 的主分区只有 200GB 不到，别说镜像文件了，
平时训练个 BERT 就该清理旧的 checkpoint 了，所以尽量少的镜像和容器，让容器能够复用对我来说相当重要)</li><li>如果有远程开发的需求，则需要管理多个容器的端口映射，即使是使用 Docker-Compose 我也不觉得有多方便。
更何况还要专门记下每个项目使用的容器对应的端口号。</li></ul></li></ol><h3 id=亿点废话>亿点废话</h3><p>曾经在网络上搜过多版本共存的问题，答案不多，能数得过来的基本都是第一种修改环境变量的方式。
当时网络上并没见到有人提到第二种方式，不过第二种也并不是我本人直接想到的。
只是一次巧合，我发现两个 conda 环境分别装了 tensorflow-gpu-1.14 和 tensorflow-gpu-2.1.0，
而且两个都能正常使用 GPU，于是专门查了下才确认这两个版本都是支持我们服务器上装着的 CUDA-10.0。</p><p>虽说是巧合，但也算是临时的解决方案吧，确实能一定程度上解决问题，也那么用了很长时间，
直到研究依赖了 <code>TensorFlow-GPU>=2.4.0</code> 的 <a href=https://github.com/google-research/text-to-text-transfer-transformer>text-to-text-transfer-transformer</a>。
当时由于时间关系，就简单了解了 TensorFlow 官方文档上通过 Docker 使用 TensorFlow 的方式临时搭了个环境。</p><p>有一说一，虽然因为公司严格的网络规则导致各种各样麻烦的问题，以至于环境搭建非常困难，
但在环境独立的容器根本不用考虑太多环境、库冲突等问题，还是很舒服的。</p><h2 id=事前准备>事前准备</h2><ol><li>首先需要确认你的显卡是支持 CUDA 的 NVIDIA 显卡，在 <a href=https://developer.nvidia.com/cuda-gpus>支持 CUDA® 的 GPU 卡</a> 可以查看</li><li>安装 <a href=https://www.nvidia.com/drivers>NVIDIA® GPU 驱动程序</a></li></ol><h3 id=安装-docker-engine>安装 Docker Engine</h3><p>参照 <a href=https://docs.docker.com/engine/install/>Install Docker Engine | Docker Documentation</a> 操作即可</p><h3 id=安装-docker-compose>安装 Docker-Compose</h3><p>Docker-Compose 可以方便我们管理多个容器，非常推荐使用！</p><ol><li>下载 Docker-Compose 的二进制文件</li></ol><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh><span class=ln>1</span>sudo curl -L <span class=s2>&#34;https://github.com/docker/compose/releases/download/1.29.1/docker-compose-</span><span class=k>$(</span>uname -s<span class=k>)</span><span class=s2>-</span><span class=k>$(</span>uname -m<span class=k>)</span><span class=s2>&#34;</span> -o /usr/local/bin/docker-compose
</code></pre></div><ol start=2><li>添加可执行权限</li></ol><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh><span class=ln>1</span>sudo chmod +x /usr/local/bin/docker-compose
</code></pre></div><h3 id=安装-nvidia-container-toolkit>安装 nvidia-container-toolkit</h3><p>要让 TensorFlow 能够在 Docker 容器中使用 GPU，需要在启动容器时指定 <code>--gpus</code> 参数,
而这个参数需要有 nvidia-container-toolkit 支持才行。</p><ol><li>设置 Nvidia-Docker 安装源</li></ol><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh><span class=ln>1</span><span class=nv>distribution</span><span class=o>=</span><span class=k>$(</span>. /etc/os-release<span class=p>;</span><span class=nb>echo</span> <span class=nv>$ID$VERSION_ID</span><span class=k>)</span> <span class=se>\
</span><span class=ln>2</span><span class=se></span>    <span class=o>&amp;&amp;</span> wget --no-check-certificate -qO - https://nvidia.github.io/nvidia-docker/gpgkey <span class=p>|</span> sudo apt-key add - <span class=se>\
</span><span class=ln>3</span><span class=se></span>    <span class=o>&amp;&amp;</span> wget --no-check-certificate -qO - https://nvidia.github.io/nvidia-docker/<span class=nv>$distribution</span>/nvidia-docker.list <span class=p>|</span> sudo tee /etc/apt/sources.list.d/nvidia-docker.list
</code></pre></div><ol start=2><li>更新软件源并安装 nvidia-container-toolkit</li></ol><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh><span class=ln>1</span>sudo apt-get update <span class=o>&amp;&amp;</span> sudo apt-get install -y nvidia-container-toolkit
</code></pre></div><ol start=3><li>重启 Docker</li></ol><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh><span class=ln>1</span>sudo systemctl restart docker 
</code></pre></div><p>这一步单看 TensorFlow 的文档是看不出这一步的，由于没有经验，再加上公司网络安全的原因，
折腾了好久，最终答案可以参考 <a href=https://github.com/NVIDIA/nvidia-docker/issues/1296#issuecomment-646521766>这个 Issue</a>。
虽然指令和 issue 里的有点区别，其实安装思路基本是固定的。</p><h4 id=烦人的坑>烦人的坑</h4><p>这里也算是为了自己吧，记录一下因为网络踩到的坑：
公司由于 Linux 系统无法安装公司要求的证书，<code>https://</code> 链接均无法正常访问，参考以下几种对策：</p><ul><li>使用 <code>wget --no-check-certificate</code> 下载证书</li><li>在 windows 下好证书再拷贝到 linux 机器上执行 <code>sudo apt-key</code></li><li>设置代理，本质其实和第二个一样</li></ul><h3 id=下载-tensorflow-的-docker-镜像>下载 TensorFlow 的 Docker 镜像</h3><p>这里以 <code>tensorflow-gpu-1.15.5</code> 和 <code>tensorflow-gpu-latest</code> 为例：</p><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh><span class=ln>1</span>docker pull tensorflow/tensorflow:1.15.5-gpu
<span class=ln>2</span>docker pull tensorflow/tensorflow:latest-gpu
</code></pre></div><h2 id=骚操作的正式开始>骚操作的正式开始</h2><h3 id=编写-dockerfile>编写 Dockerfile</h3><p>自己重做镜像主要是为了以后重建容器方便，TensorFlow1 和 TensorFlow2 都做镜像，
内容除了作成的镜像名和端口映射外完全一致。</p><div class=highlight><pre class=chroma><code class=language-dockerfile data-lang=dockerfile><span class=ln> 1</span><span class=k>FROM</span><span class=s> tensorflow/tensorflow:1.15.5-gpu-py3</span><span class=err>
</span><span class=ln> 2</span><span class=err>
</span><span class=ln> 3</span><span class=err></span><span class=c># 这个脚本主要是安装软件，详见下文</span><span class=err>
</span><span class=ln> 4</span><span class=err></span><span class=k>COPY</span> ./init_env1.sh ./sources.list /<span class=err>
</span><span class=ln> 5</span><span class=err></span><span class=k>RUN</span> chmod u+x /init_env*.sh <span class=err>
</span><span class=ln> 6</span><span class=err></span><span class=k>RUN</span> /init_env1.sh<span class=err>
</span><span class=ln> 7</span><span class=err>
</span><span class=ln> 8</span><span class=err></span><span class=c># 这个脚本主要是添加用户、设置 ssh</span><span class=err>
</span><span class=ln> 9</span><span class=err></span><span class=k>COPY</span> ./init_env2.sh /<span class=err>
</span><span class=ln>10</span><span class=err></span><span class=k>RUN</span> chmod u+x /init_env*.sh <span class=err>
</span><span class=ln>11</span><span class=err></span><span class=k>RUN</span> /init_env2.sh<span class=err>
</span><span class=ln>12</span><span class=err>
</span><span class=ln>13</span><span class=err></span><span class=k>EXPOSE</span><span class=s> 22</span><span class=err>
</span><span class=ln>14</span><span class=err></span><span class=c># 设置一个挂载目录，将所有数据挂载到宿主机</span><span class=err>
</span><span class=ln>15</span><span class=err></span><span class=k>VOLUME</span><span class=s> /AI</span><span class=err>
</span><span class=ln>16</span><span class=err></span><span class=k>WORKDIR</span><span class=s> /AI</span><span class=err>
</span><span class=ln>17</span><span class=err></span><span class=c># 将默认用户修改为新建的用户</span><span class=err>
</span><span class=ln>18</span><span class=err></span><span class=k>USER</span><span class=s> ai</span><span class=err>
</span></code></pre></div><p><em><strong>Note:</strong></em>
<em>Dockerfile 的编写应<strong>尽量避免过多的使用 COPY, RUN 等指令，能合并就合并</strong>。
因为每一行指令都会创建一个 layer，layer 越多，作成的镜像越大</em>
<em>我这里分开这么多写是因为编写该镜像过程中，<code>init_env1.sh</code> 更新源、安装软件太耗时间。
且 <code>init_env2.sh</code> 有多次修改，而 <code>init_env1.sh</code> 没怎么修改过，为了节约镜像打包的时间专门分开处理。</em></p><h3 id=dockerfile-需要用到的脚本>Dockerfile 需要用到的脚本</h3><h4 id=init_env1sh>init_env1.sh</h4><p>用于安装之后可能用到的相关软件，因为 TensorFlow 提供的镜像并没有这些。
可根据需要修改、添加其他软件。</p><div class=highlight><pre class=chroma><code class=language-shell data-lang=shell><span class=ln>1</span>cp /sources.list /etc/apt/sources.list
<span class=ln>2</span>rm -Rf /var/lib/apt/lists/* 
<span class=ln>3</span><span class=c1># 更新源和软件之前，一定要禁止 CUDA 的更新（不然要这 TensorFlow 的镜像有何用）</span>
<span class=ln>4</span>apt-mark hold cuda*
<span class=ln>5</span>apt-mark hold machine-learning
<span class=ln>6</span>apt-get update 
<span class=ln>7</span><span class=c1># 安装所需软件</span>
<span class=ln>8</span>apt-get -y install openssh-server openssh-sftp-server vim sudo net-tools
</code></pre></div><h4 id=init_env2sh>init_env2.sh</h4><p>用于设置 SSH 以及登录用户</p><div class=highlight><pre class=chroma><code class=language-shell data-lang=shell><span class=ln> 1</span><span class=nv>config_file</span><span class=o>=</span><span class=s2>&#34;/etc/ssh/sshd_config&#34;</span>
<span class=ln> 2</span><span class=nv>sftp_server</span><span class=o>=</span><span class=s2>&#34;/usr/lib/openssh/sftp-server&#34;</span>
<span class=ln> 3</span>
<span class=ln> 4</span><span class=c1># modify configs start</span>
<span class=ln> 5</span><span class=c1># 禁止 root 用户通过 ssh 登录</span>
<span class=ln> 6</span><span class=k>if</span> grep -q <span class=s2>&#34;^PermitRootLogin&#34;</span> <span class=nv>$config_file</span><span class=p>;</span><span class=k>then</span>
<span class=ln> 7</span>   sed -i <span class=s1>&#39;/^PermitRootLogin/s/yes/no/&#39;</span> <span class=nv>$config_file</span>
<span class=ln> 8</span><span class=k>else</span>
<span class=ln> 9</span>   sed -i <span class=s1>&#39;$a PermitRootLogin no&#39;</span> <span class=nv>$config_file</span>
<span class=ln>10</span><span class=k>fi</span>
<span class=ln>11</span>
<span class=ln>12</span><span class=c1># 这里设置是为了让 WinSCP 等客户端可以通过 sftp 连接容器</span>
<span class=ln>13</span><span class=k>if</span> grep -q <span class=s2>&#34;^Subsystem&#34;</span> <span class=nv>$config_file</span><span class=p>;</span><span class=k>then</span>
<span class=ln>14</span>   sed -i <span class=s1>&#39;/^Subsystem/s/$sftp_server/internal-sftp/&#39;</span> <span class=nv>$config_file</span>
<span class=ln>15</span><span class=k>else</span>
<span class=ln>16</span>   sed -i <span class=s1>&#39;$a Subsystem  sftp  internal-sftp&#39;</span> <span class=nv>$config_file</span>
<span class=ln>17</span><span class=k>fi</span>
<span class=ln>18</span><span class=c1># modify configs end</span>
<span class=ln>19</span>
<span class=ln>20</span><span class=c1># 添加用户:ai，设置用户目录:/AI，修改密码:123456</span>
<span class=ln>21</span><span class=c1># 注意了解 `useradd` 和 `adduser` 的区别</span>
<span class=ln>22</span>useradd -d /AI -U -m ai <span class=o>&amp;&amp;</span> <span class=nb>echo</span> <span class=s2>&#34;ai:123456&#34;</span> <span class=p>|</span> chpasswd
<span class=ln>23</span><span class=c1># 为 ai 用户设置 sudo 权限</span>
<span class=ln>24</span>sed -i <span class=s1>&#39;$a ai   ALL=(ALL) NOPASSWD: ALL&#39;</span> /etc/sudoers
</code></pre></div><p><em><strong>Note:</strong> 为什么不使用 root 用户？因为即使是在 Docker 容器里，直接使用 root 用户也是存在安全隐患的，
如果容器挂载了宿主机的目录，那么在容器内使用 root 用户操作挂载目录实际上和在宿主机使用 root 用户操作该目录是一样。
就连 TensorFlow 的容器也会在你进入容器时判断当前用户的身份，如果是 root 则会建议你使用非 root 用户</em></p><h4 id=sourceslist>sources.list</h4><p>软件源设置，这里用的阿里云，当然也可以替换成别的源。</p><div class=highlight><pre class=chroma><code class=language-text data-lang=text><span class=ln> 1</span>deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse
<span class=ln> 2</span>deb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse
<span class=ln> 3</span>deb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse
<span class=ln> 4</span>deb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse
<span class=ln> 5</span>deb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse
<span class=ln> 6</span>deb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse
<span class=ln> 7</span>deb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse
<span class=ln> 8</span>deb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse
<span class=ln> 9</span>deb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse
<span class=ln>10</span>deb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse
</code></pre></div><h3 id=打包镜像>打包镜像</h3><p>在 Dockerfile 所在目录执行：</p><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh><span class=ln>1</span>docker build . -t tf:1.15.5
</code></pre></div><h3 id=新建容器并启动>新建容器并启动</h3><p>通过以下 Docker 命令就可以启动支持 GPU 的容器</p><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh><span class=ln>1</span>docker run -it --name tf1 --gpus all -p 10022:22 -v /home/ai/tf_project:/AI tf:1.15.5
</code></pre></div><p>如果只需要一个虚拟环境/一个版本的 TensorFlow，到这里就已经足够了，但这并不是本文的重点。
接下来继续讲骚操作，技巧其实很简单：活用 <code>VOLUME</code> 挂载目录。</p><h3 id=安装-minicondaanaconda>安装 Miniconda/Anaconda</h3><p>conda 安装很简单，从<a href=https://docs.conda.io/en/master/miniconda.html>官网</a>下载对应脚本，
添加可执行权限后执行，按照交互提示操作就行，这里不再赘述。</p><h3 id=使用-docker-compose-管理容器>使用 Docker-Compose 管理容器</h3><p>※ 假设装好的 Miniconda 所在根目录为 <code>/home/ai/miniconda</code></p><ol><li>创建任意目录作为项目目录，比如 <code>/home/ai/ai_docker</code></li><li>目录下新建文件 <code>docker-compose.yml</code>，和 <code>Dockerfile</code> 一样，文件名固定</li></ol><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=ln> 1</span><span class=c># 由于大多数都是 Docker 指令中能看到的，功能相同，下面只会对一些特殊的指令做解释。</span><span class=w>
</span><span class=ln> 2</span><span class=w></span><span class=c># 注意这是本文件使用的 Docker-Compose 版本，决定你能在这个文件使用哪些指令</span><span class=w>
</span><span class=ln> 3</span><span class=w></span><span class=nt>version</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;3.9&#34;</span><span class=w>
</span><span class=ln> 4</span><span class=w>
</span><span class=ln> 5</span><span class=w></span><span class=nt>services</span><span class=p>:</span><span class=w>
</span><span class=ln> 6</span><span class=w>  </span><span class=c># 服务名，随意</span><span class=w>
</span><span class=ln> 7</span><span class=w>  </span><span class=nt>tensorflow1</span><span class=p>:</span><span class=w>
</span><span class=ln> 8</span><span class=w>    </span><span class=c># 指定镜像</span><span class=w>
</span><span class=ln> 9</span><span class=w>    </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>tf:1.15.5</span><span class=w>
</span><span class=ln>10</span><span class=w>    </span><span class=nt>container_name</span><span class=p>:</span><span class=w> </span><span class=l>tf1</span><span class=w>
</span><span class=ln>11</span><span class=w>    </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span><span class=ln>12</span><span class=w>      </span>- <span class=s2>&#34;10022:22&#34;</span><span class=w>
</span><span class=ln>13</span><span class=w>    </span><span class=nt>restart</span><span class=p>:</span><span class=w> </span><span class=l>always</span><span class=w>
</span><span class=ln>14</span><span class=w>    </span><span class=c># 指定用户，在下一节环境变量配置</span><span class=w>
</span><span class=ln>15</span><span class=w>    </span><span class=nt>user</span><span class=p>:</span><span class=w> </span><span class=l>${CURRENT_UID}</span><span class=w>
</span><span class=ln>16</span><span class=w>    </span><span class=c># 使容器保持后台运行</span><span class=w>
</span><span class=ln>17</span><span class=w>    </span><span class=nt>tty</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span><span class=ln>18</span><span class=w>    </span><span class=nt>networks</span><span class=p>:</span><span class=w>
</span><span class=ln>19</span><span class=w>      </span>- <span class=l>ai-bridge</span><span class=w>
</span><span class=ln>20</span><span class=w>    </span><span class=c># 使容器能使用 GPU</span><span class=w>
</span><span class=ln>21</span><span class=w>    </span><span class=nt>deploy</span><span class=p>:</span><span class=w>
</span><span class=ln>22</span><span class=w>      </span><span class=nt>resources</span><span class=p>:</span><span class=w>
</span><span class=ln>23</span><span class=w>        </span><span class=nt>reservations</span><span class=p>:</span><span class=w>
</span><span class=ln>24</span><span class=w>          </span><span class=nt>devices</span><span class=p>:</span><span class=w>
</span><span class=ln>25</span><span class=w>          </span>- <span class=nt>driver</span><span class=p>:</span><span class=w> </span><span class=l>nvidia</span><span class=w>
</span><span class=ln>26</span><span class=w>            </span><span class=nt>capabilities</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>gpu]</span><span class=w>
</span><span class=ln>27</span><span class=w>    </span><span class=c># 目录挂载，骚的根源</span><span class=w>
</span><span class=ln>28</span><span class=w>    </span><span class=nt>volumes</span><span class=p>:</span><span class=w>
</span><span class=ln>29</span><span class=w>      </span><span class=c># 可以直接将原来的用户目录先挂载过来，授权 ro</span><span class=w>
</span><span class=ln>30</span><span class=w>      </span><span class=c># 目的：如果有需要和容器环境一起使用但不依赖 TensorFlow 版本的软件，如：个别分词器，</span><span class=w>
</span><span class=ln>31</span><span class=w>      </span><span class=c># 直接装在宿主机即可，容器内只需要配置环境变量</span><span class=w>
</span><span class=ln>32</span><span class=w>      </span>- <span class=l>/home/ai:/home/ai:ro</span><span class=w>
</span><span class=ln>33</span><span class=w>      </span><span class=c># 将宿主机的 Miniconda 目录挂载到容器的同等目录下，授权 rw</span><span class=w>
</span><span class=ln>34</span><span class=w>      </span><span class=c># 目的：共享宿主机的 conda 环境，不同项目只要依赖的 TensorFlow 版本相同，</span><span class=w>
</span><span class=ln>35</span><span class=w>      </span><span class=c># 就只需要创建 conda 环境克隆已有环境的 TensorFlow，在新环境装项目依赖即可。</span><span class=w>
</span><span class=ln>36</span><span class=w>      </span><span class=c># </span><span class=w>
</span><span class=ln>37</span><span class=w>      </span><span class=c># 此外，远程开发甚至可以不为任何容器分配端口映射，配合下一条挂载项，直接连接宿主机的项目</span><span class=w>
</span><span class=ln>38</span><span class=w>      </span><span class=c># 和 conda 环境即可。当然，如果没什么意外，前面安装 SSH 等软件的步骤都可以省去。</span><span class=w>
</span><span class=ln>39</span><span class=w>      </span>- <span class=l>/home/ai/miniconda3:/home/ai/miniconda3:rw</span><span class=w>
</span><span class=ln>40</span><span class=w>      </span><span class=c># 将项目目录挂载到容器的默认工作目录，同时这里也是前面指定的、容器里的用户目录。</span><span class=w>
</span><span class=ln>41</span><span class=w>      </span>- <span class=l>/home/ai/tensorflow_volume:/AI</span><span class=w>
</span><span class=ln>42</span><span class=w>
</span><span class=ln>43</span><span class=w>  </span><span class=c># 与上一节仅服务名、镜像、端口映射不同，其他一致。添加新容器时也一样</span><span class=w>
</span><span class=ln>44</span><span class=w>  </span><span class=nt>tensorflow2</span><span class=p>:</span><span class=w>
</span><span class=ln>45</span><span class=w>    </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>tf:latest</span><span class=w>
</span><span class=ln>46</span><span class=w>    </span><span class=nt>container_name</span><span class=p>:</span><span class=w> </span><span class=l>tf2</span><span class=w>
</span><span class=ln>47</span><span class=w>    </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span><span class=ln>48</span><span class=w>      </span>- <span class=s2>&#34;20022:22&#34;</span><span class=w>
</span><span class=ln>49</span><span class=w>    </span><span class=nt>restart</span><span class=p>:</span><span class=w> </span><span class=l>always</span><span class=w>
</span><span class=ln>50</span><span class=w>    </span><span class=nt>user</span><span class=p>:</span><span class=w> </span><span class=l>${CURRENT_UID}</span><span class=w>
</span><span class=ln>51</span><span class=w>    </span><span class=nt>tty</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span><span class=ln>52</span><span class=w>    </span><span class=nt>networks</span><span class=p>:</span><span class=w>
</span><span class=ln>53</span><span class=w>      </span>- <span class=l>ai-bridge</span><span class=w>
</span><span class=ln>54</span><span class=w>    </span><span class=nt>deploy</span><span class=p>:</span><span class=w>
</span><span class=ln>55</span><span class=w>      </span><span class=nt>resources</span><span class=p>:</span><span class=w>
</span><span class=ln>56</span><span class=w>        </span><span class=nt>reservations</span><span class=p>:</span><span class=w>
</span><span class=ln>57</span><span class=w>          </span><span class=nt>devices</span><span class=p>:</span><span class=w>
</span><span class=ln>58</span><span class=w>          </span>- <span class=nt>driver</span><span class=p>:</span><span class=w> </span><span class=l>nvidia</span><span class=w>
</span><span class=ln>59</span><span class=w>            </span><span class=nt>capabilities</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>gpu]</span><span class=w>
</span><span class=ln>60</span><span class=w>    </span><span class=nt>volumes</span><span class=p>:</span><span class=w>
</span><span class=ln>61</span><span class=w>      </span>- <span class=l>/home/ai:/home/ai:ro</span><span class=w>
</span><span class=ln>62</span><span class=w>      </span>- <span class=l>/home/ai/miniconda3:/home/ai/miniconda3:rw</span><span class=w>
</span><span class=ln>63</span><span class=w>      </span>- <span class=l>/home/ai/tensorflow_volume:/AI</span><span class=w>
</span><span class=ln>64</span><span class=w>
</span><span class=ln>65</span><span class=w></span><span class=nt>networks</span><span class=p>:</span><span class=w>
</span><span class=ln>66</span><span class=w>  </span><span class=nt>ai-bridge</span><span class=p>:</span><span class=w>
</span><span class=ln>67</span><span class=w>    </span><span class=nt>driver</span><span class=p>:</span><span class=w> </span><span class=l>bridge</span><span class=w>
</span></code></pre></div><ol start=3><li>目录下新建文件 <code>.env</code></li></ol><div class=highlight><pre class=chroma><code class=language-ini data-lang=ini><span class=ln>1</span><span class=na>写入当前用户的 uid 和 gid</span>
<span class=ln>2</span><span class=na>CURRENT_UID</span><span class=o>=</span><span class=s>1000:1000</span>
</code></pre></div><p><em><strong>Note:</strong> 如何查看当前用户的 id？执行 <code>id -u</code> 和 <code>id -g</code> 就好了</em></p><ol start=4><li>启动容器</li></ol><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh><span class=ln>1</span><span class=nb>cd</span> /home/ai/ai_docker
<span class=ln>2</span>docker-compose up -d
</code></pre></div><p>使用 <code>docker-compose ps</code> 查看容器状态，是 <code>Up</code> 就启动成功了。</p><h2 id=骚操作的后续补充>骚操作的后续补充</h2><h3 id=为容器初始化-conda>为容器初始化 conda</h3><p>虽然完全可以进入容器后再激活对应的 conda 环境，但用得久了着实感觉繁琐。
可以通过用户登录会自动调用 <code>.bashrc</code> 或 <code>.profile</code> 等脚本的机制来实现。</p><ol><li>关闭 conda 的自动初始化环境
因为上面配置了容器的 conda 环境实际上是和宿主机共用的，为了避免登录脚本激活容器后 conda 又切换回默认环境，首先关闭该设置</li></ol><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh><span class=ln>1</span>cat <span class=s>&lt;&lt;EOF &gt;&gt;~/.condarc
</span><span class=ln>2</span><span class=s>auto_activate_base: false
</span><span class=ln>3</span><span class=s>EOF</span>
</code></pre></div><ol start=2><li>修改宿主机 <code>.bashrc</code>，添加环境切换函数
安装 conda (并启用自动初始化)后，在宿主机的 <code>.bashrc</code> 找到类似下面的代码</li></ol><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh><span class=ln> 1</span><span class=c1># &gt;&gt;&gt; conda initialize &gt;&gt;&gt;</span>
<span class=ln> 2</span><span class=c1># !! Contents within this block are managed by &#39;conda init&#39; !!</span>
<span class=ln> 3</span><span class=nv>__conda_setup</span><span class=o>=</span><span class=s2>&#34;</span><span class=k>$(</span><span class=s1>&#39;/home/ai/miniconda3/bin/conda&#39;</span> <span class=s1>&#39;shell.bash&#39;</span> <span class=s1>&#39;hook&#39;</span> 2&gt; /dev/null<span class=k>)</span><span class=s2>&#34;</span>
<span class=ln> 4</span><span class=k>if</span> <span class=o>[</span> <span class=nv>$?</span> -eq <span class=m>0</span> <span class=o>]</span><span class=p>;</span> <span class=k>then</span>
<span class=ln> 5</span>    <span class=nb>eval</span> <span class=s2>&#34;</span><span class=nv>$__conda_setup</span><span class=s2>&#34;</span>
<span class=ln> 6</span><span class=k>else</span>
<span class=ln> 7</span>    <span class=k>if</span> <span class=o>[</span> -f <span class=s2>&#34;/home/ai/miniconda3/etc/profile.d/conda.sh&#34;</span> <span class=o>]</span><span class=p>;</span> <span class=k>then</span>
<span class=ln> 8</span>        . <span class=s2>&#34;/home/ai/miniconda3/etc/profile.d/conda.sh&#34;</span>
<span class=ln> 9</span>    <span class=k>else</span>
<span class=ln>10</span>        <span class=nb>export</span> <span class=nv>PATH</span><span class=o>=</span><span class=s2>&#34;/home/ai/miniconda3/bin:</span><span class=nv>$PATH</span><span class=s2>&#34;</span>
<span class=ln>11</span>    <span class=k>fi</span>
<span class=ln>12</span><span class=k>fi</span>
<span class=ln>13</span><span class=nb>unset</span> __conda_setup
<span class=ln>14</span><span class=c1># &lt;&lt;&lt; conda initialize &lt;&lt;&lt;</span>
</code></pre></div><p>在上述代码之后添加</p><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh><span class=ln> 1</span>tf1<span class=o>(){</span>
<span class=ln> 2</span>  <span class=k>if</span> <span class=o>[</span> -z <span class=s2>&#34;</span><span class=nv>$1</span><span class=s2>&#34;</span> <span class=o>]</span><span class=p>;</span> <span class=k>then</span>
<span class=ln> 3</span>    <span class=nv>env</span><span class=o>=</span>tf1
<span class=ln> 4</span>  <span class=k>else</span>
<span class=ln> 5</span>    <span class=nv>env</span><span class=o>=</span><span class=nv>$1</span>
<span class=ln> 6</span>  <span class=k>fi</span>
<span class=ln> 7</span>  docker <span class=nb>exec</span> -it -e <span class=nv>env</span><span class=o>=</span><span class=nv>$1</span> -e <span class=nv>TF_VERSION</span><span class=o>=</span>TensorFlow1 tf1 bash
<span class=ln> 8</span><span class=o>}</span>
<span class=ln> 9</span>tf2<span class=o>(){</span>
<span class=ln>10</span>  <span class=k>if</span> <span class=o>[</span> -z <span class=s2>&#34;</span><span class=nv>$1</span><span class=s2>&#34;</span> <span class=o>]</span><span class=p>;</span> <span class=k>then</span>
<span class=ln>11</span>    <span class=nv>env</span><span class=o>=</span>tf2
<span class=ln>12</span>  <span class=k>else</span>
<span class=ln>13</span>    <span class=nv>env</span><span class=o>=</span><span class=nv>$1</span>
<span class=ln>14</span>  <span class=k>fi</span>
<span class=ln>15</span>  docker <span class=nb>exec</span> -it -e <span class=nv>env</span><span class=o>=</span><span class=nv>$1</span> -e <span class=nv>TF_VERSION</span><span class=o>=</span>TensorFlow2 tf2 bash
<span class=ln>16</span><span class=o>}</span>
</code></pre></div><ol start=3><li>配置宿主机默认 conda 环境
在宿主机 <code>.bashrc</code> 最后添加</li></ol><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh><span class=ln>1</span>conda activate base
</code></pre></div><ol start=4><li>在容器 <code>.bashrc</code> 添加</li></ol><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh><span class=ln> 1</span><span class=c1># &gt;&gt;&gt; auto activate env &gt;&gt;&gt;</span>
<span class=ln> 2</span><span class=k>if</span> <span class=o>[</span> -n <span class=s2>&#34;</span><span class=nv>$env</span><span class=s2>&#34;</span> <span class=o>]</span><span class=p>;</span> <span class=k>then</span>
<span class=ln> 3</span>  conda activate <span class=nv>$env</span>  <span class=c1># 进入容器自动激活指定的 conda 环境 </span>
<span class=ln> 4</span><span class=k>else</span>
<span class=ln> 5</span>  <span class=c1># 如果没有指定 conda 环境，则自动进入容器自身 tf 版本对应的 conda 环境</span>
<span class=ln> 6</span>  <span class=nv>tf1</span><span class=o>=</span><span class=sb>`</span>pip freeze <span class=p>|</span> grep tensorflow-gpu<span class=o>==</span>1.15.5<span class=sb>`</span>  <span class=c1># 查询当前容器默认环境的 tf1 版本，1.15.5 修改成自己安装的版本</span>
<span class=ln> 7</span>  <span class=nv>tf2</span><span class=o>=</span><span class=sb>`</span>pip freeze <span class=p>|</span> grep tensorflow-gpu<span class=o>==</span>2.1.0<span class=sb>`</span>  <span class=c1># 查询当前容器默认环境的 tf1 版本，2.1.0 修改成自己安装的版本</span>
<span class=ln> 8</span>  <span class=k>if</span> <span class=o>[</span> -n <span class=s2>&#34;</span><span class=nv>$tf1</span><span class=s2>&#34;</span> <span class=o>]</span><span class=p>;</span> <span class=k>then</span>
<span class=ln> 9</span>    conda activate tf1
<span class=ln>10</span>  <span class=k>elif</span> <span class=o>[</span> -n <span class=s2>&#34;</span><span class=nv>$tf2</span><span class=s2>&#34;</span> <span class=o>]</span><span class=p>;</span> <span class=k>then</span>
<span class=ln>11</span>    conda activate tf2
<span class=ln>12</span>  <span class=k>fi</span>
<span class=ln>13</span><span class=k>fi</span>
<span class=ln>14</span><span class=c1># &lt;&lt;&lt; auto activate env &lt;&lt;&lt;</span>
</code></pre></div><p>至此，通过 <code>tf1</code> 和 <code>tf2</code> 就能直接进入容器并激活相对的 conda 环境了。
举个例子：</p><ul><li>进入 TensorFlow1 容器：<code>tf1</code></li><li>进入 TensorFlow2 容器：<code>tf2</code></li><li>进入 TensorFlow1 容器，并激活名为 <code>BERT</code> 的虚拟环境：<code>tf1 BERT</code></li><li>进入 TensorFlow2 容器，并激活名为 <code>T5</code> 的虚拟环境：<code>tf1 T5</code></li></ul><h3 id=简单的使用说明>简单的使用说明</h3><p>TODO</p><h3 id=使用-vscode-远程开发>使用 VSCode 远程开发</h3><p>安装插件 <code>Remote - SSH</code> 连接宿主机即可，即使不查资料也不难操作。
由于写本文的时间已经太晚了而且这一节不算重点，暂且告一段落了。。。</p><h2 id=一些已知存在但仍不知如何解决的小问题>一些已知存在但仍不知如何解决的小问题</h2><p>TODO</p><div class=post-ref><h2 id=参考链接>参考链接</h2><ul><li><a href=https://docs.docker.com/engine/install/>Install Docker Engine | Docker Documentation</a></li><li><a href=https://docs.docker.com/compose/install/>Install Docker Compose | Docker Documentation</a></li><li><a href=https://www.tensorflow.org/install/docker>Docker | TensorFlow</a></li></ul></div></div><div class=post-copyright><p class=copyright-item><span>Author:</span>
<span>niceRAM</span></p><p class=copyright-item><span>Link:</span>
<a href=https://niceram.xyz/2021/04/15/20210415_2157/>https://niceram.xyz/2021/04/15/20210415_2157/</a></p><p class="copyright-item lincese">本文采用<a rel=license href=http://creativecommons.org/licenses/by-nc/4.0/ target=_blank>知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可</p></div><div class=post-tags><section><i class="iconfont icon-tag"></i>Tag(s):
<span class=tag><a href=https://niceram.xyz/tags/docker/>#docker</a></span>
<span class=tag><a href=https://niceram.xyz/tags/miniconda/>#miniconda</a></span>
<span class=tag><a href=https://niceram.xyz/tags/anaconda/>#anaconda</a></span>
<span class=tag><a href=https://niceram.xyz/tags/tensorflow/>#tensorflow</a></span>
<span class=tag><a href=https://niceram.xyz/tags/tensorflow-gpu/>#tensorflow-gpu</a></span>
<span class=tag><a href=https://niceram.xyz/tags/ubuntu/>#ubuntu</a></span>
<span class=tag><a href=https://niceram.xyz/tags/nvidia/>#Nvidia</a></span>
<span class=tag><a href=https://niceram.xyz/tags/cuda/>#CUDA</a></span>
<span class=tag><a href=https://niceram.xyz/tags/cudnn/>#cudnn</a></span>
<span class=tag><a href=https://niceram.xyz/tags/cuda-%E5%A4%9A%E7%89%88%E6%9C%AC%E5%85%B1%E5%AD%98/>#CUDA 多版本共存</a></span></section><section><a href=javascript:window.history.back();>back</a></span> ·
<span><a href=https://niceram.xyz/>home</a></span></section></div><div class=post-nav><a href=https://niceram.xyz/2021/03/25/20210325_1319/ class=prev rel=prev title="使用 GitHub Actions 自动构建·部署静态网站到 GitHub Pages"><i class="iconfont icon-left"></i>&nbsp;使用 GitHub Actions 自动构建·部署静态网站到 GitHub Pages</a>
<a href=https://niceram.xyz/2021/07/07/20210707_1441/ class=next rel=next title="Angular 获取原生 DOM 对象时出现 'undefined'">Angular 获取原生 DOM 对象时出现 'undefined'&nbsp;<i class="iconfont icon-right"></i></a></div><div class=post-social><div class=social-links><style type=text/css>@font-face{font-family:iconfont;src:url(//at.alicdn.com/t/font_1910062_4213q7gpkpy.eot);src:url(//at.alicdn.com/t/font_1910062_4213q7gpkpy.eot?#iefix)format('embedded-opentype'),url(//at.alicdn.com/t/font_1910062_4213q7gpkpy.woff2)format('woff2'),url(//at.alicdn.com/t/font_1910062_4213q7gpkpy.woff)format('woff'),url(//at.alicdn.com/t/font_1910062_4213q7gpkpy.ttf)format('truetype'),url(//at.alicdn.com/t/font_1910062_4213q7gpkpy.svg#iconfont)format('svg')}</style><a href="https://space.bilibili.com/10929768/channel/detail?cid=12977" target=_blank rel="me noopener"><i class="iconfont icon-bilibili"></i></a><a href=https://github.com/niceRAM target=_blank rel="me noopener"><i class="iconfont icon-github"></i></a><a href=https://twitter.com/niceRAM95 target=_blank rel="me noopener"><i class="iconfont icon-twitter"></i></a><a href=mailto:niceniceram@gmail.com rel="me noopener"><i class="iconfont icon-mail01"></i></a><a href=https://t.me/niceRAM target=_blank rel="me noopener"><i class="iconfont icon-telegram"></i></a></div></div><div class=post-comment><div id=gitalk-container></div><link rel=stylesheet href=https://cdn.bootcss.com/gitalk/1.5.2/gitalk.css><script src=https://cdn.bootcss.com/gitalk/1.5.2/gitalk.min.js></script><script>const gitalk=new Gitalk({clientID:'e2ac76297f1e7ec232f9',clientSecret:'4385d32dd8c6ab5f97856bbe2e04a1e05a237276',repo:'blog-comments',owner:'niceRAM',admin:['niceRAM'],proxy:'https://cors-anywhere-niceblog.herokuapp.com/https://github.com/login/oauth/access_token',id:location.pathname,distractionFreeMode:!1});(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('gitalk-container').innerHTML='Gitalk comments not available by default when the website is previewed locally.';return}gitalk.render('gitalk-container')})()</script></div></article></div></main><footer class=footer><div class=copyright>&copy;
<span itemprop=copyrightYear>2018 - 2021</span>
<span class=rotating>🌸</span>
<span class=author itemprop=copyrightHolder><a href=https://niceram.xyz/>niceRAM</a> |</span>
<span>Powered by <a href=https://gohugo.io/ target=_blank rel="external nofollow">Hugo</a> & <a href=https://github.com/niceRAM/nicesima target=_blank rel="external nofollow">nicesima</a></span></div></footer><script src=https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js crossorigin=anonymous></script><script defer src=/js/vendor_main.min.js></script><script src=https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin=anonymous></script><script>pangu.spacingPage()</script></div></body></html>